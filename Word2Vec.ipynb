{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##**Skip-gram model from scratch using a sample corpus**"
      ],
      "metadata": {
        "id": "SQ4_GB1venjr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7qcaddSeZx9P"
      },
      "outputs": [],
      "source": [
        "corpus = [\n",
        "    'he is a king is',\n",
        "    'she is a queen',\n",
        "    'he is a man',\n",
        "    'she is a woman',\n",
        "    'warsaw is poland capital',\n",
        "    'berlin is germany capital',\n",
        "    'paris is france capital',\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_corpus(corpus):\n",
        "    tokens = [x.split() for x in corpus]\n",
        "    return tokens\n",
        "\n",
        "tokenized_corpus = tokenize_corpus(corpus)\n",
        "\n",
        "corpus_list = []\n",
        "for sentence in tokenized_corpus:\n",
        "    for token in sentence:\n",
        "      corpus_list.append(token)"
      ],
      "metadata": {
        "id": "6TjiI-SXZ3ID"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary = []\n",
        "for sentence in tokenized_corpus:\n",
        "    for token in sentence:\n",
        "        if token not in vocabulary:\n",
        "            vocabulary.append(token)\n",
        "\n",
        "word2idx = {w: idx for (idx, w) in enumerate(vocabulary)}\n",
        "idx2word = {idx: w for (idx, w) in enumerate(vocabulary)}\n",
        "\n",
        "vocabulary_size = len(vocabulary)"
      ],
      "metadata": {
        "id": "QY2koYwnZ3wo"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Skip-gram Model**"
      ],
      "metadata": {
        "id": "Z3tSY2vk6GCF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "\n",
        "class SkipGram:\n",
        "    def __init__(self, corpus, vocab_size, embedding_size, neg_sampling_rate, window_size, learning_rate=0.01):\n",
        "        self.corpus = corpus\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_size = embedding_size\n",
        "        self.neg_sampling_rate = neg_sampling_rate\n",
        "        self.window_size = window_size\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        # Initialize the word vectors randomly\n",
        "        self.word_vectors = np.random.randn(vocab_size, embedding_size)\n",
        "\n",
        "        # Initialize the context vectors to zeros\n",
        "        self.context_vectors = np.random.randn(vocab_size, embedding_size)\n",
        "        \n",
        "        # Initialize the biases to zeros\n",
        "        self.word_biases = np.zeros(shape=(vocab_size,))\n",
        "        self.context_biases = np.zeros(shape=(vocab_size,))\n",
        "\n",
        "    # Define sigmoid function\n",
        "    def sigmoid(self, x):\n",
        "      return 1 / (1 + np.exp(-x)) \n",
        "\n",
        "    # Build the word frequency table for negative sampling\n",
        "    def get_negative_prob(self, corpus):\n",
        "      word_freq = defaultdict(int)\n",
        "      for word in self.corpus:\n",
        "        word_freq[word] += 1\n",
        "        \n",
        "      total_word_freq = sum(word_freq.values())\n",
        "      word_probs = {word: freq / total_word_freq for word, freq in word_freq.items()}\n",
        "\n",
        "      # Negative sampling probabilities\n",
        "      noise_dist = {key: val ** (3/4) for key, val in word_probs.items()}\n",
        "      Z = sum(noise_dist.values())\n",
        "      noise_dist_normalized = {key: val / Z for key, val in noise_dist.items()}\n",
        "      return noise_dist_normalized\n",
        "\n",
        "    def get_negative_samples(self, context_word):\n",
        "      negative_samples = []\n",
        "      noise_dist_normalized = self.get_negative_prob(corpus)\n",
        "\n",
        "      while len(negative_samples) < self.neg_sampling_rate:\n",
        "        sample_list = np.random.choice(list(noise_dist_normalized.keys()), size = self.neg_sampling_rate, p=list(noise_dist_normalized.values()))\n",
        "        for sample in sample_list:\n",
        "          if sample != context_word and sample not in negative_samples:\n",
        "            negative_samples.append(sample)\n",
        "            return negative_samples\n",
        "        \n",
        "        \n",
        "    def train(self, num_epochs):\n",
        "        for epoch in tqdm(range(num_epochs)):\n",
        "            loss = 0\n",
        "            \n",
        "            for i, word in enumerate(self.corpus):\n",
        "                # Get the context words for this center word\n",
        "                context_words = self.corpus[max(0, i - self.window_size) : i] + self.corpus[i + 1 : i + self.window_size + 1]\n",
        "                center_word_id = word2idx[word]\n",
        "                \n",
        "                # Loop over each context word and update the embeddings\n",
        "                for context_word_i in context_words:\n",
        "                    context_word_id = word2idx[context_word_i]\n",
        "                    # Perform negative sampling to get negative samples\n",
        "                    negative_samples = self.get_negative_samples(context_word_i)\n",
        "                    \n",
        "                    # Update the center word vector and bias\n",
        "                    center_vector = self.word_vectors[center_word_id]\n",
        "                    center_bias = self.word_biases[center_word_id]\n",
        "                    context_vector = self.context_vectors[context_word_id]\n",
        "                    context_bias = self.context_biases[context_word_id]\n",
        "                    \n",
        "                    pos_score = np.dot(center_vector, np.transpose(context_vector)) + center_bias + context_bias\n",
        "                    pos_score_grad = self.sigmoid(pos_score) - 1\n",
        "                    \n",
        "                    center_vector_grad = pos_score_grad * context_vector\n",
        "                    center_bias_grad = pos_score_grad\n",
        "                    context_vector_grad = pos_score_grad * center_vector\n",
        "                    context_bias_grad = pos_score_grad\n",
        "                    \n",
        "                    loss -= np.log(self.sigmoid(pos_score))\n",
        "                    \n",
        "                    # Loop over each negative sample and update the embeddings\n",
        "                    for negative_sample in negative_samples:\n",
        "                        negative_sample_id = word2idx[negative_sample]\n",
        "                        negative_vector = self.context_vectors[negative_sample_id]\n",
        "                        negative_bias = self.context_biases[negative_sample_id]\n",
        "                        \n",
        "                        neg_score = np.dot(center_vector, np.transpose(negative_vector)) + center_bias + negative_bias\n",
        "                        neg_score_grad = self.sigmoid(neg_score)\n",
        "                        \n",
        "                        center_vector_grad += neg_score_grad * negative_vector\n",
        "                        center_bias_grad += neg_score_grad\n",
        "                        negative_vector_grad = neg_score_grad * center_vector\n",
        "                        negative_bias_grad = neg_score_grad\n",
        "                        \n",
        "                        loss -= np.log(self.sigmoid(-neg_score))\n",
        "                        \n",
        "                        # Update the negative sample context vector and bias\n",
        "                        self.context_vectors[negative_sample_id] -= self.learning_rate * negative_vector_grad\n",
        "                        self.context_biases[negative_sample_id] -= self.learning_rate * negative_bias_grad\n",
        "                        \n",
        "                    # Update the center word vector and bias\n",
        "                    self.word_vectors[center_word_id] -= self.learning_rate * center_vector_grad\n",
        "                    self.word_biases[center_word_id] -= self.learning_rate * center_bias_grad\n",
        "\n",
        "            print(f\"Loss after epoch {epoch}: {loss / len(corpus)}\")\n",
        "\n",
        "        return self, self.word_vectors\n",
        "\n"
      ],
      "metadata": {
        "id": "1F8AWvYdGZ9f"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SkipGram(corpus_list, vocab_size = vocabulary_size, embedding_size = 3, neg_sampling_rate=2, window_size=2, learning_rate=0.01).train(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nnne-m-bbTBx",
        "outputId": "b376060e-00bf-40ea-b6cf-329d6d3b016a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5/5 [00:00<00:00, 96.40it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss after epoch 0: 25.311524390080763\n",
            "Loss after epoch 1: 24.868888011461735\n",
            "Loss after epoch 2: 23.53942109438085\n",
            "Loss after epoch 3: 23.190190106772867\n",
            "Loss after epoch 4: 25.45003104618511\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<__main__.SkipGram at 0x7f9698e90a90>,\n",
              " array([[-0.59385423,  0.51233554,  1.06558124],\n",
              "        [-0.79228443, -0.44279977, -0.62379612],\n",
              "        [ 0.22536055,  1.00681264,  0.29579073],\n",
              "        [ 1.45673005,  1.28858807, -0.9740576 ],\n",
              "        [ 0.46564592, -1.10070552,  0.18261535],\n",
              "        [ 0.62612754, -1.58964195, -0.28565742],\n",
              "        [-1.0365158 , -1.65562567, -0.54116416],\n",
              "        [-0.22185968,  0.96660497,  0.70797113],\n",
              "        [ 0.95787363,  0.47526893,  1.56466027],\n",
              "        [-0.64947276,  0.48866358,  0.18961807],\n",
              "        [ 0.27125167,  0.92446307,  0.27966786],\n",
              "        [ 0.49710557,  0.22309339,  0.24039582],\n",
              "        [-1.004651  , -1.21505755, -0.17143081],\n",
              "        [-0.14698578,  1.96582328,  0.31332156],\n",
              "        [-0.94319637, -1.29849078, -0.66970296]]))"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    }
  ]
}